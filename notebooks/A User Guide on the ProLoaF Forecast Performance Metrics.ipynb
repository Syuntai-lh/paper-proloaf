{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast Metrics:\n",
    "The motivation of this notebook is to bring together all the metrics generally used for forecasting.<br>\n",
    "While the first half discusses the point forecast metrics, the second half moves on to address the probabilistic ones to give a complete picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with point metrics and from now,we indicate predicted variable by $\\hat{y_i}$, observed variable by $y_i$ and the dataset size by $N$.\n",
    "\n",
    "## Point forecast metrics:\n",
    "#### Mean Error(ME): \n",
    "* Gives us the average deviations between observed and predicted values.$$ ME(y_i,\\hat{y_i}) = \\frac{1}{N} \\sum_{i=0}^{N}(y_i - \\hat{y_i}) $$\n",
    "\n",
    "* Helps us to know the model's bias to under or overforecast.\n",
    "* **<u>Best</u>:** Should be close to zero.<br>\n",
    "* **<u>Drawbacks</u>:** Positive and Negative errors can neutralize each other and result into zero.Hence we will have no idea of the distribution of errors.\n",
    "\n",
    "#### Mean Absolute Error(MAE): \n",
    "\n",
    "* Gives average magnitude of forecast errors. $$ MAE(y_i,\\hat{y_i}) = \\frac{1}{N} \\sum_{i=0}^{N}|y_i - \\hat{y_i}| $$\n",
    "* Helps us to know the width of error distribution.\n",
    "* **<u>Best</u>:** Should be close to zero.<br>\n",
    "\n",
    "* Sometimes,it is useful to quantify the tails of the error distribution,as higher forecasting errors can cause higher disurbances. If we want to give more importance to higher forecasting errors, we can also use Mean Squared Error(MSE) defined as  \n",
    "$$ MSE(y_i,\\hat{y_i}) = \\frac{1}{N} \\sum_{i=0}^{N}(y_i - \\hat{y_i})^2 $$\n",
    ">Unless you are optimizing using this function, it might not be that useful, since we cant directly interpret it.\n",
    "* In that case, one may want to take the square root of the above called as Root Mean Squared Error(RMSE) defined as below\n",
    "$$ RMSE(y_i,\\hat{y_i}) = \\sqrt{\\frac{1}{N} \\sum_{i=0}^{N}(y_i - \\hat{y_i})^2 }$$\n",
    ">However it still lacks the interpretation as it just doesnot describe the average error alone and is dependent on the distribution of the squared error rather than the error.\n",
    "\n",
    "***For assessing forecast across a single series, in general,<u> MAE</u> is preferred as it is easier to interpret and compute.\n",
    "However, we cannot use it to compare forecast accuracy between various series because it is scale dependent.***\n",
    "\n",
    "**So,it might be helpful, to have a quick glance at the scale independent metrics.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale independent metrics:\n",
    "Source: https://robjhyndman.com/publications/accuracy-intermittent-demand/\n",
    "pdf: https://robjhyndman.com/papers/foresight.pdf\n",
    "\n",
    "#### Percentage error metrics:\n",
    "Percentage error is usually given by $\\displaystyle PE(y_i,\\hat{y_i}) =100\\times\\frac{(y_i - \\hat{y_i})} {y_i} $\n",
    "<br>\n",
    "We can actually see many papers evaluating their based on the percentage error metrics like MAPE and sMAPE.\n",
    "The most popular one among business practioners being scale-independent is **Mean Absolute percentage error(MAPE)**\n",
    "which can be read as the average of the absolute percentage errors defined above.$$\n",
    "MAPE(y_i,\\hat{y_i}) = 100\\times \\frac{1}{N} \\sum_{i=0}^{N}|\\frac{(y_i - \\hat{y_i})} {y_i}| = \\frac{1}{N} \\sum_{i=0}^{N}|PE|\n",
    "$$\n",
    "\n",
    "However, it has it's own shortcomings as below:\n",
    "* It becomes undefined when the actual value is 0.\n",
    "* When the actual values are close to zero, it becomes extremely skewed and if we try to optimize on it, it may most likely undershoot the actual value.\n",
    "* Also,it is asymmetric, it puts heavier penalty on those that exceed the actual than those that are lower than the actual, for the same amount of error.\n",
    "\n",
    "There is also another metric called  **symmetric Mean Absolute percentage error(sMAPE)** mostly used in forecasting competitions.It was basically proposed to counter the boundlessness problem in MAPE.The absolute difference between $y_i$ and $\\hat{y_i}$ is divided by half the sum of absolute values of the actual value $y_i$ and the forecast value $\\hat{y_i}$\n",
    "$$\n",
    "sMAPE(y_i,\\hat{y_i}) =\\frac{100 \\%}{N} \\sum_{i=1}^{N} \\frac{\\left|\\hat{y_i}-y_i\\right|}{\\left(\\left|y_i\\right|+\\left|\\hat{y_i}\\right|\\right) / 2}\n",
    "$$\n",
    "\n",
    "But it still is asymmetrical(Refer to this wikipedia [example](https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error) for more).Generally, it is recommended not to use any percentage error based metrics when both forecast and true values can be zero.Also in case where variables can take both positive and negative variables.\n",
    "\n",
    "#### Relative error based metrics:\n",
    "\n",
    "Their principle is to divide error for each sample by some benchmark model, which generally is a naive model. \n",
    "Considering $\\hat{{y_i}_b}$ as the forecast by some base line model, Relative absolute error can defined as \n",
    "$$RAE = \\frac{1}{N} \\sum_{i=1}^{N}|\\frac{(y_i - \\hat{y_i})}{(y_i -\\hat{{y_i}_b}) }| $$\n",
    "\n",
    "They also suffer from the upper bound limit problem, if the baselines model perfectly perfectly fits the target value.\n",
    "\n",
    "#### Scale free error metrics:\n",
    "\n",
    "Introduced by [Hyndman and Koehler](https://robjhyndman.com/publications/another-look-at-measures-of-forecast-accuracy/), they involve simply scaling the errors based on the insample MAE from a naive forecast method.\n",
    "For non seasonal time series, Scaled error is defined as $$ Scaled error(y_i,\\hat{y_i}) = \\frac{(y_i - \\hat{y_i})}{\\frac{1}{T-1}\\sum_{t=2}^{T}|y_t - y_{t-1}|} $$ <br>where $T$ is the length of the insample for that forecast. This implies $ y_{T+1} = y_i$<br>\n",
    "<br>\n",
    "For seasonal time series, a scaled error can be defined using seasonal naïve forecasts as follows:\n",
    " $$ Scaled error(y_i,\\hat{y_i}) = \\frac{(y_i - \\hat{y_i})}{\\frac{1}{T-m}\\sum_{t=m+1}^{T}|y_t - y_{t-m}|} $$ where $m$ is the seasonal frequency.\n",
    "<br>\n",
    "* Because the numerator and denominator both involve values on the scale of the original data, this metrics is independent of the scale of the data.\n",
    "* A scaled error is less than one if it arises from a better forecast than the average naïve forecast computed on the training data. Conversely, it is greater than one if the forecast is worse than the average naïve forecast computed on the training data.\n",
    "\n",
    "The metric generally used in the class of scaled error metrics is **Mean Scaled Error Metric(MASE)** which is simply mean of abosulte value of the scaled metric defined above.\n",
    "\n",
    "$$ MASE(y_i,\\hat{y_i}) = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{|y_i - \\hat{y_i}|}{\\frac{1}{T-1}\\sum_{t=2}^{T}|y_t - y_{t-1}|} $$\n",
    "\n",
    "\n",
    "\n",
    "* Sometimes, there are chances of becoming infinite when all historical data are equal. \n",
    "* It has to be assumed that the period-to-period difference of the time series is stationary, so that the scaling factor is a consistent estimator of the scale of the series.\n",
    "\n",
    "This completes our overview on the point forecast metrics.Lets move to the ones,we are intereseted in, the probabilistic forecasts.<br>\n",
    "<br>\n",
    "**Please note that all these are error metrics and hence lower the better. Also one can easily extend them to multi step forecasting, by calculating errors for each step in the forecast horizon and taking average on the whole.**\n",
    "\n",
    "## Probabilistic forecast metrics:\n",
    "\n",
    "* Probabilistic forecasts corresponds to the estimation of the statistical distribution of a future event and hence generate forecasts represented by predictive distributions which can be defined by Probability density function(PDF) or a cumulative distribution function(CDF), where CDF of a random variable X evaluated at x is $\\displaystyle CDF_X(x) = Probability(X \\leq x)$\n",
    "\n",
    "* Probabilistic forecasts can be generated by nonparametric or parametric approaches. \n",
    "    * Nonparametric methods do not make any assumptions about the shape of the density and concentrate on finding a set of quantile values by minimizing quantile/pinball loss for those quantiles. More of these discrete estimates helps to summarize the CDF.\n",
    "    * On the other hand, parametric methods assume a parametric distribution and seek parameters that optimize the predictive distribution by minimizing loss like negative log likelihood of that distribution.\n",
    "    \n",
    "#### Numerical scores:\n",
    "* One may present the evaluation of their probabilistic forecasts in terms of numerical score like CRPS,Ignorance score,Mean Interval Score,Quantile score etc.\n",
    "A score is said to be proper when perfect forecasts are given best score value and these scores have to proper to be sure of the validity of the results.\n",
    "\n",
    "#### Quantile score:\n",
    "Suppose, we want to estimate to conditional median or other quantiles, this score can be used to evaluate the loss function, also called pinball loss.\n",
    "> * Recall that  a quantile $q_{\\tau}$ indicates that there is a probability ${\\tau}$ that the target variable occurs below $q_{\\tau}$\n",
    "* For a random variable $y$ with cumulative distribution function $F$ is defined as $q_{\\tau}=F^{-1}(\\tau)=\\inf \\{y: F(y) \\geqslant \\tau\\}$ where $\\tau \\in[0,1]$\n",
    "    * Without any assumption on the distribution,using these set of quantiles spanning the unit interval, we can get discrete estimates of predictive CDF.\n",
    "    * Also Prediction Intervals can be calculated.\n",
    "    \n",
    "To estimate a quantile $\\tau \\in[0,1]$, with true value $y_i$ and the predicted value at that quantile $\\hat{y_i}$,the quantile loss is defined as\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "L_{\\tau}(y, \\hat{y})= \\bigg[(\\tau-1)\\sum_{i\\in y_{i}<\\hat{y}_{i}}\\left|y_{i}-\\hat{y}_{i}\\right|\\bigg]+\\bigg[\\tau\\sum_{i \\in y_{i} \\geq \\hat{y}_{i}}\\left|y_{i}-\\hat{y}_{i}\\right|\\bigg]\n",
    "$$\n",
    "and this has to be divided by the length of the dataset $N$ \n",
    "<br>\n",
    "* When $\\tau = 0.5$, that is penalizing the estimates both above and below the true value equally, we get median.\n",
    "* For lower quantiles(or the ones which are expected to appear less probably), the estimates which are above true value are highly penalized than the others and for the higher quantiles, the estimates which are below the true value are highly penalized than the others,which is what we expect.  \n",
    "* It is negatively oriented, lower the better.\n",
    "* Please note that it is non-differentiable at 0.\n",
    "\n",
    "#### Interval score\n",
    "\n",
    "Proposed by [Gneiting and Raftery](https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2007.00587.x), the interval score helps us to calibrate the quality of prediction intervals.\n",
    "Since, we now know that we can estimate quantiles using the above quantile loss function, we can then very well deduce the prediction intervals.\n",
    "><u>Just a quick recall to Prediction intervals</u>:\n",
    "* Prediction intervals(PI) define the range of values within which an observation is expected to be with a certain probability which is generally referred to as nominal coverage rate.For a significance level $\\alpha$, the coverage rate is given by $(1-\\alpha)100\\%$ \n",
    "* To determin PI, it is necessary to choose the way it should be centered on the PDF as stated in this [paper](https://onlinelibrary.wiley.com/doi/abs/10.1002/we.230) by Prof.Pinson and collegaues and it is generally centered on the median.\n",
    "*  A central PI with a coverage rate of $(1-\\alpha) 100 \\%$ is estimated by using the quantile $\\left(\\hat{q}_{\\tau = \\alpha / 2}\\right)$ as the lower bound and the $(1-\\alpha / 2)$ quantile $\\left(\\hat{q}_{\\tau = 1-\\alpha / 2}\\right)$ as the upper bound.\n",
    "* So, a PI with $(1-\\alpha) 100 \\%$ nominal coverage rate is given by\n",
    "$\\widehat{P I}_{(1-\\alpha) 100\\%}=\\left[\\widehat{q}_{\\tau=\\alpha / 2}, \\hat{q}_{\\tau =1-\\alpha / 2}\\right]$\n",
    "\n",
    "So the interval score is defined as follows:\n",
    "$$\n",
    "IS_{\\alpha} = \\bigg[\\left ( U_{i}-L_{i}\\right)\\bigg]+\\bigg[\\frac{2}{a}\\left(L_{i}-y_{i}\\right) {\\Large\\unicode{x1D7D9}} \\left\\{y_{i}<L_{i}\\right\\}\\bigg]+\\bigg[\\frac{2}{a}\\left(y_{i}-U_{i}\\right) {\\Large\\unicode{x1D7D9}}\\left\\{y_{i}>U_{i}\\right\\}\\bigg]\n",
    "$$\n",
    "\n",
    "   * where $U_{i}$ and $L_{i}$ represent upper and lower prediction bounds which can be estimated either by quantiles as stated above or from the estimated parameters like standard deviation e.g. $\\hat{y}_{i} \\pm c \\hat{\\sigma}$ where $c$ is the coverage probability.<br>\n",
    "   * $y_i$ is the future observations of the series, $\\alpha$ is the significance level and ${\\Large\\unicode{x1D7D9}}$ is the indicator function (being 1 if $y_i$is within the postulated interval and 0 otherwise).For a $95\\%$ prediction interval,$\\alpha = 0.05$\n",
    "   \n",
    "So, while the last two parts of the score indicate the penalty for the estimates where the future points are outside bounds the first part of the score indicates the width of the prediction interval and adds to the penalty as we want it to be low.\n",
    "\n",
    "* Taking the mean of the above Interval score gives us the **Mean Interval score(MIS)**, which were using in our analysis.\n",
    "* The lower the better.\n",
    "\n",
    "\n",
    "#### Continuous ranked probability score(CRPS):\n",
    "[CRPS](https://journals.ametsoc.org/doi/full/10.1175/1520-0434%282000%29015%3C0559%3ADOTCRP%3E2.0.CO%3B2)\n",
    "is just the mean square error (MSE) of your predicted cumulative density function (CDF) and the true CDF.It is defined as follows:\n",
    "\n",
    "$$\n",
    "CRPS=\\frac{1}{N} \\sum_{i=1}^{N} \\int_{-\\infty}^{+\\infty}\\left[\\hat{F}_{y_{pred}}^{i}(y)-F_{y_{obs}}^{i}(y)\\right]^{2} dy\n",
    "$$\n",
    "where $\\hat{F}_{y_{pred}}(y)$ is the predictive CDF of the target variable $y$ and $F_{y_{obs}}(y)$ is a cumulative-probability step function that jumps from 0 to 1 at the point where the forecast variable $x$ equals the observation $\\left.y_{obs} \\text { (i.e.} F_{y_{obs}}(y)=1_{\\left[y \\geq y_{obs}\\right]}\\right)$\n",
    "\n",
    "* This metric notably differs from simpler metrics such as MAE because of its asymmetric expression: while the forecasts are probabilistic, the observations are deterministic. \n",
    "* Unlike the pinball loss function, the CRPS does not focus on any specific point of the probability distribution but considers the distribution of the forecasts as a whole.\n",
    "* For deterministic forecasts, it turns out to be **Mean Absolute Error(MAE)**.\n",
    "* It is negatively oriented(lower values are better) and is in the same scale as the target variable.\n",
    "* The lack of closed form of the associated integral sometimes makes it difficult to apply.\n",
    "\n",
    "#### Ignorance score(IGN):\n",
    "(Referred from [link](https://journals.ametsoc.org/doi/pdf/10.1175/MWR2904.1))<br>\n",
    "(Also this is what were using as main loss function namely Negative log likelihood)<br>\n",
    "<br>\n",
    "Also called as the log score or divergence, it is simply the negative logarithm of the predictive PDF verified at the observed value $y_i$ and is defined as follows:\n",
    "$$\\operatorname{ign}(f, y_i)=-\\log {f(y_i)}$$\n",
    "\n",
    "In the case of a normal predictive PDF with mean $\\mu$ and variance $\\sigma^{2},$ we have\n",
    "\n",
    "$$\\operatorname{ign}\\left[\\mathcal{N}\\left(\\mu, \\sigma^{2}\\right), y\\right]=\\frac{1}{2} \\ln \\left(2 \\pi \\sigma^{2}\\right)+\\frac{(y-\\mu)^{2}}{2 \\sigma^{2}}$$\n",
    "\n",
    "and the average ignorance is\n",
    "$$\\\n",
    "\\begin{aligned}\n",
    "\\mathrm{IGN} &=\\frac{1}{n} \\sum_{i=1}^{n} \\operatorname{ign}\\left(F_{i}, y_{i}\\right) \\\\\n",
    "&=\\frac{1}{n} \\sum_{i=1}^{n}\\left[\\frac{1}{2} \\ln \\left(2 \\pi \\sigma_{i}^{2}\\right)+\\frac{\\left(y_{i}-\\mu_{i}\\right)^{2}}{2 \\sigma_{i}^{2}}\\right]\n",
    "\\end{aligned}$$\n",
    "\n",
    "* Both CRPS and IGN are negatively oriented scores, in that a smaller value is better, and both scores are proper, meaning that they reward honest assessments.\n",
    "* However, a key difference between the ignorance score and the continuous ranked probability score is that,CRPS grows linearly in the normalized prediction error, $z \\equiv$ $(y-\\mu) / \\sigma,$ while IGN grows quadratically in $z$ Hence, the ignorance score assigns harsh penalties to particularly poor probabilistic forecasts, and can be exceedingly sensitive to outliers and extreme events.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Properties for good quality Probabilistic forecasts :\n",
    "(Referred from [link](https://www.sciencedirect.com/science/article/abs/pii/S0038092X19310382))\n",
    "\n",
    "Besides numerical score one can also look at some desired properties of probabilistic forecasts to evaluate their methods.\n",
    "They are mainly reliability , sharpness and resolution.\n",
    "\n",
    "#### Reliability:\n",
    "\n",
    "* Reliability or calibration refers to the statistical consistency between the forecasts and the observations.\n",
    "* In other terms, the nominal coverage rate of the prediction intervals should be equal to the empirical one (e.g. a $90 \\%$ PI should cover $90 \\%$ of the observations). \n",
    "* The reliability property is an important prerequisite as non reliable forecasts would lead to a systematic blas in subsequent decislon-making processes([Source](https://onlinelibrary.wiley.com/doi/abs/10.1002/we.230))\n",
    "\n",
    "To measure reliability, there exists a metric called Prediction Interval Coverage probability(PICP) which is defined as below:\n",
    "\n",
    "$$\n",
    "\\mathrm{PICP}=\\frac{1}{N} \\sum_{i=1}^{N} c_{i} $$\n",
    "\n",
    "where $N$ is the number of samples, and $c_{i}$ is the indicator of PICP and is defined as\n",
    "$$\n",
    "c_{i}=\\left\\{\\begin{array}{ll}\n",
    "{1,} & {y_{i} \\in PI^{\\alpha}} \\\\\n",
    "{0,} & {y_{i} \\notin PI^{\\alpha}}\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "where $y_i$ is the observed value and $PI^{\\alpha}$ indicates estimated predicted interval for significance level $\\alpha$\n",
    "<br>\n",
    "It is expected that the above coverage probability PICP should asymptotically converge to the nominal confidence level given by$100(1-\\alpha)\\%$ \n",
    "\n",
    "#### Resolution:\n",
    "\n",
    "Resolution measures the capacity of a forecasting model to issue forecasts that are case-dependent.<br>\n",
    "To understand it,consider the following example\n",
    ">Imagine a distribution built from all the available past data of the parameter to forecast. The climatological forecast uses this unique distribution to forecast any future events. A high resolution forecasting system generates forecasts that differ from the climatology and, as a consequence, forecasts that are significantly different from each other. Climatological forecasts are perfectly reliable though having no resolution. Consequently, a skillful probabilistic forecasting system should issue reliable forecasts and with high resolution.\n",
    "\n",
    "<br>\n",
    "From a statistical point of view, resolution amounts to evaluate the capacity of the forecast system to produce different density forecasts depending on the forecast conditions (i.e. the predictive distributions are not only conditioned by the value of the predictand) (Pinson et al, 2007 ). For instance, the prediction intervals may exhibit increasing widths with increasing forecast horizon. Also, regarding the solar irradiance (GHI), the width of the PIs may vary according the sun's position in the sky.\n",
    "\n",
    "There's no unique mathematical way to establish Resolution measure.However, there are studies which try to study Resolution by decomposing numerical scores into uncertainty,reliability and resolution.\n",
    "\n",
    "#### Sharpness:\n",
    "* Sharpness evaluates how informative the forecasts are. Practically, sharpness refers to the concentration of the predictive distributions. \n",
    "* Sharpness is a function of the forecasts only and does not depend on the observations. Consequently, a forecasting system can produce sharp forecasts yet being useless if those probabilistic forecasts are not reliable.\n",
    "\n",
    "* A probabilistic forecast is sharp if prediction intervals are shorter on average than prediction intervals derived from naive methods, such as climatology or persistence.\n",
    "\n",
    "As per [Pinson et.al]((https://onlinelibrary.wiley.com/doi/abs/10.1002/we.230)),the sharpness of the predictive distributions can be assessed by calculating the mean size of the central prediction intervals.\n",
    "<br>\n",
    "Sharpness is given by\n",
    "$$ \\text Sharpness_\\alpha =\\frac{1}{N} \\sum_{i=1}^{N}\\left(\\hat{U_i}-\\hat{L_i}\\right)$$\n",
    "where $\\hat{U_i}$ and $\\hat{L_i}$ upper and lower bounds estimated for a signifcance level $alpha$ either by quantile methods or from parameters of a predictive distribution.\n",
    "\n",
    "* Lower the sharpness value, the better forecasts. \n",
    "\n",
    "Here ends our high level discussion on forecasting metrics,there may be other metrics that are being used, but most of them broadly come under these types and having a good understanding of the above can help in comprehending any new metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: All the metrics discussed above can be extended for multi step forecasting as well and are generally expected to become worse as we move further in the forecast horizon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the above discussed metrics, we are currently using RMSE, Sharpness, Reliability and Mean Interval Score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
