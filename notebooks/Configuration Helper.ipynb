{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "## Main Config\n",
    "ProLoaF's configuration file is written using JSON. As such, whitespace is allowed and ignored in your syntax.\n",
    "We mostly use strings, boolean, and ``null``, to specify the paths, and parameters of our targeted forecasting project. \n",
    "Numbers and booleans should be  unquoted.\n",
    "\n",
    "For better readability, we assume in the examples below, \n",
    "that your working directory is set to the main project path of the cloned \n",
    "[repository](https://git.rwth-aachen.de/acs/public/automation/plf/plf-training).\n",
    "\n",
    "### Generating a Configuration File\n",
    "\n",
    "A *new* config file can be generated automatically by using:\n",
    "```sh\n",
    "python source\\config_maker.py --new targets/<STATION-NAME>/config.json\n",
    "```\n",
    "or manually.\n",
    "\n",
    "To *modify* the file, you can change parameters with our helper or apply modifications manually. \n",
    "When using the helper, your modifications must be set in `config_maker.py`. You can then run:\n",
    "\n",
    "```sh\n",
    "python source\\config_maker.py --mod targets/<STATION-NAME>/config.json\n",
    "```\n",
    "\n",
    "### Configuration Loading\n",
    "\n",
    "The default location of the main configuration file is `./targets/` or better `./targets/<STATION>`. \n",
    "The best practice is to generate sub-directories for each forecasting exercise, i.e. a new *station*. \n",
    "As the project originated from electrical load forecasting on substation-level, \n",
    "the term *station* or *target-station* is used to refer to the city or substation identifier\n",
    "from which the measurement data is originating.   \n",
    "Most of the example scripts in ProLoaF use the config file for training and evaluation, \n",
    "as it serves as a central place for parametrization.\n",
    "\n",
    "At this stage you should have the main config file for your forecasting project: `./targets/<STATION>/config.json`.\n",
    "[plf-util](https://git.rwth-aachen.de/acs/public/automation/plf/plf-util) comes with basic functions to parse, \n",
    "edit and store the config file. We make use of this when calling e.g. our example training script: \n",
    "\n",
    "```sh\n",
    "$ python source\\fc_train.py -s gefcom2017/nh_data\n",
    "```\n",
    "\n",
    "The flag ``-s`` allows us to specify the station name (=target directory) through the string that follows, \n",
    "i.e. *gefcom2017/nh_data*. The 'train' script will expect and parse the config.json given in the target directory.\n",
    "<!---\n",
    "```sh\n",
    "from utils.config_util import read_config, parse_with_loss\n",
    "MAIN_PATH = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))\n",
    "sys.path.append(MAIN_PATH)\n",
    "\n",
    "ARGS, LOSS_OPTIONS = parse_with_loss()\n",
    "    PAR = read_config(model_name=ARGS.station, config_path=ARGS.config, main_path=MAIN_PATH)\n",
    "```\n",
    "--->\n",
    "You can also manually specify the path to the config file by adding ``-c <CONFIG_PATH>`` to the above mentioned\n",
    "statement.\n",
    "\n",
    "> **_Note:_** If not otherwise specified, during training, per default the neural network maximizes the \n",
    "[Maximum Likelihood Estimation of Gaussian Parameters](http://jrmeyer.github.io/machinelearning/2017/08/18/mle.html), \n",
    "for a 95% prediction interval. \n",
    "This so-called loss criterion can be changed to any metric that quantifies the (probabilistic) performance \n",
    "of the forecast. A common non-parametric option is the quantile loss.\n",
    "You can apply quantile loss criterion as follows:\n",
    "\n",
    "```sh\n",
    "    $ python source\\fc_train.py -s gefcom2017/nh_data --quantiles 0.025 0.975\n",
    "```\n",
    "> Here we have specified the 95% prediction interval, by setting ``q1=0.025`` and ``q2=0.975``.\n",
    "\n",
    "See more detailed descriptions and further loss options in the full [list of parameters](#parameter-list).\n",
    "\n",
    "### Path Settings\n",
    "\n",
    "Through the config file the user specifies the data source location, and the directories for logging, \n",
    "exporting performance analyses and most importantly, the trained RNN model binary.\n",
    "\n",
    "**Path Specs:**\n",
    "```json\n",
    "{\n",
    "    \"data_path\": \"./data/<FILE-NAME>.csv\",\n",
    "    \"evaluation_path\": \"./oracles/eval_<MODEL-NAME>/\",\n",
    "    \"output_path\": \"./oracles/\",\n",
    "    \"exploration_path\": \"./targets/sege/tuning.json\", \n",
    "    \"log_path\": \"./logs/\"\n",
    "}\n",
    "```\n",
    "The output-, exploration- and log- paths may stay unchanged, but the data path and evaluation path must be specified.\n",
    "\n",
    "> **_Note:_** The data path should contain a csv file that includes all input data column-wise in any time-resolution.\n",
    "In our example train-& evaluation scripts, the first column is treated as datetime information and declared as \n",
    "pandas datetime index. *oracles* is the default naming of the output directory, \n",
    ">in which the prediction model and predictive performance are stored.  \n",
    "\n",
    "### Timeseries Settings\n",
    "\n",
    "ProLoaF is a machine-learning based timeseries forecasting project. The supervised learning requires data with \n",
    "a (pandas) datetime index. Typical time resolutions are:`ms`, `s`, `m`, `h`, `d`.\n",
    "Endogenous (lagged) inputs and exogenous (=explanatory) variables that affect the future explained variable, \n",
    "are split into multiple windows with the sequence length of ``history_horizon`` and fed to the encoder.\n",
    "For better understanding, we recommend \n",
    "[the illustrative guide](https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb) \n",
    "on a similar sequence-to-sequence architecture authored by Ben Trevett.\n",
    "\n",
    "The time step size is equal to the underlying timeseries data resolution. \n",
    "The example files apply day-ahead forecasting in hourly resolution. \n",
    "> **_Note:_** A forecast that produces a predicted sequence starting from the forecasting execution time and including the next day, \n",
    "is >=24h, depending on the forecast execution time t, e.g. a day-ahead forecast executed at 9 am, shall produce a 40 hour horizon.\n",
    "\n",
    "Following parameters configure the input- and output- sequence length:\n",
    "```json\n",
    "{\n",
    "  \"history_horizon\": 42,\n",
    "  \"forecast_horizon\": 24\n",
    "}\n",
    "```\n",
    "### Data Partitioning\n",
    "\n",
    "In machine learning, we typically split available data to train the model and test its performance. \n",
    "With the training set, the model is parameterized. By checking against validation data, we track the fitting process.\n",
    "In a final step, the test set serves to assess and confirm the predictive power of the trained model. \n",
    "To configure the size of each mentioned set, specify:\n",
    "\n",
    "- **train_split**:\n",
    "Given an input dataframe df, all timestamps before the specified split are used for training: \n",
    "``df[:train_split*df.shape[0]]``.\n",
    "\n",
    "- **validation_split**:\n",
    "For validation during training, we use all data between train and validation limiters in the dataframe df:\n",
    "``df[train_split*df.shape[0]:validation_split*df.shape[0]]``.\n",
    "\n",
    "> **_Note:_** The *test_split* is set per default, through the remaining input data from df: \n",
    ">``df[validation_split*df.shape[0]:]``. \n",
    "  \n",
    "### Data Pre-processing through Scaling\n",
    "  \n",
    "### Feature Selection\n",
    "\n",
    "```json\n",
    "    \"feature_groups\": [\n",
    "        {\n",
    "            \"name\": \"main\",\n",
    "            \"scaler\": [\n",
    "                \"robust\",\n",
    "                15,\n",
    "                85\n",
    "            ],\n",
    "            \"features\": [\n",
    "                \"<COLUMN-IDENTIFIER-1>\"\n",
    "            ]\n",
    "        }\n",
    "```\n",
    "\n",
    "```json\n",
    "        {\n",
    "            \"name\": \"add\",\n",
    "            \"scaler\": [\n",
    "                \"minmax\",\n",
    "                -1.0,\n",
    "                1.0\n",
    "            ],\n",
    "            \"features\": [\n",
    "                \"<COLUMN-IDENTIFIER-2>\"\n",
    "            ]\n",
    "        }\n",
    "```\n",
    "\n",
    "```json\n",
    "        {\n",
    "            \"name\": \"aux\",\n",
    "            \"scaler\": null,\n",
    "            \"features\": [\n",
    "                \"<COLUMN-IDENTIFIER-3>\",\n",
    "                \"<COLUMN-IDENTIFIER-4>\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "```\n",
    "``` \"encoder_features\": [\n",
    "        \"<COLUMN-IDENTIFIER-1>\",\n",
    "        \"<COLUMN-IDENTIFIER-2>\"\n",
    "    ],\n",
    "    \"decoder_features\": [\n",
    "        \"<COLUMN-IDENTIFIER-3>\",\n",
    "        \"<COLUMN-IDENTIFIER-4>\"\n",
    "    ],\n",
    "\n",
    "### RNN Cell Type\n",
    "- GRU: trains typically faster (per epoch) with similar results compared to LSTM cells.\n",
    "- LSTM\n",
    "```json\n",
    "{\n",
    "    \"history_horizon\": 42,\n",
    "    \"forecast_horizon\": 24\n",
    "}\n",
    "```\n",
    "\n",
    "### Hyperparameters and Tuning\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"max_epochs\": 1,\n",
    "  \"batch_size\": 2,\n",
    "  \"learning_rate\": 0.0001,\n",
    "  \"core_layers\": 1,\n",
    "  \"rel_linear_hidden_size\": 1.0,\n",
    "  \"rel_core_hidden_size\": 1.0,\n",
    "  \"dropout_fc\": 0.4,\n",
    "  \"dropout_core\": 0.3\n",
    "}\n",
    "```\n",
    "\n",
    "- **max_epochs**:....\n",
    "\n",
    "- **batch_size**: ...\n",
    "\n",
    "- **learning_rate**: ...\n",
    "\n",
    "- **core_layers**: ...\n",
    "\n",
    "- **rel_linear_hidden_size**: ...\n",
    "\n",
    "- **rel_core_hidden_size**: ...\n",
    "\n",
    "- **dropout_fc**: ...\n",
    "\n",
    "- **dropout_core**: ...\n",
    "  \n",
    "Configure which hyperparameters are optimized and specify each parameters search space through a separate \n",
    "[tuning config](#tuning-config).\n",
    "> **_Note:_** Best practice is to save the tuning.config in the same directory in which the main config is given. \n",
    ">However, by setting a [specific exploration_path](#path-settings), the user can direct to a \n",
    ">different location on the machine.\n",
    "\n",
    "### GPU Specs\n",
    "Some text on cuda id\n",
    "\n",
    "### Selecting the best model\n",
    "- **best_loss**: \n",
    "- **best_score**: \n",
    "\n",
    "### Parameter List\n",
    "\n",
    "The following table summarizes the default parameters of the main config file:\n",
    "\n",
    "**Config Params**\n",
    "\n",
    "| Parameter   |      Data Type      |  Value Range |\n",
    "|:-------:|:-------------:|:-------:|\n",
    "| history_horizon | int |  > 0 |\n",
    "| forecast_horizon | int |  > 0 |\n",
    "\n",
    "**Shell Params Upon Script Execution**\n",
    "\n",
    "| Parameter   |      Data Type      |  Value Range |\n",
    "|:-------:|:-------------:|:-------:|\n",
    "| --<flag_loss> |  string in shell | {mse, rmse, } |\n",
    "\n",
    "## Tuning Config\n",
    "\n",
    "Some text on suggest-functions and common hyperparams.\n",
    "also on the duration.\n",
    "\n",
    "## Preprocessing Config\n",
    "[...]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
